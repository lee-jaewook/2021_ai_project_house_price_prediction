{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-jaewook/2021_ai_project_house_price_prediction/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C428YgV8EHVn"
      },
      "source": [
        "# 인공신경망을 이용한 주택가격지수 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY5moLgPEKdx",
        "outputId": "04780772-f92c-4150-9b59-370ec21a2102"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40kW4kz6EHVs",
        "outputId": "cd935095-7b62-4e43-e2f7-a496d4ac9a92"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "import pydot\n",
        "import graphviz \n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "import math\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmRmuLXREHVs"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/AI 2021/Dataset/house_dataset(micro_macro).csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vCQiOR3EHVt",
        "outputId": "fc490efa-c729-44b7-de45-ce4758233d1b"
      },
      "source": [
        "for i in range(df.shape[0]):\n",
        "    df[\"Date\"][i]=df[\"Date\"][i][0:4]+df[\"Date\"][i][6:-1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX85fKsDEHVt"
      },
      "source": [
        "X = np.array(df.iloc[:,1:-1])\n",
        "y = np.array(df.iloc[:,-1])"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aAAQwkfOQnx",
        "outputId": "8c0bff90-1e9a-412a-b583-9fdfd9b3bbe0"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(168, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2QHVm2EHVu"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X=scaler.transform(X)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_FsPIYvEHVu"
      },
      "source": [
        "### Train : 0~143( ~ 2018)\n",
        "### test : 144~ (2019 ~)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKaRE1N8EHVu"
      },
      "source": [
        "X_macro_train=X[:144,:6]\n",
        "X_macro_test=X[144:,:6]\n",
        "X_micro_train=X[:144,6:]\n",
        "X_micro_test=X[144:,6:]\n",
        "\n",
        "y_train = y[:144]\n",
        "y_test = y[144:]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE2pT27iEHVu"
      },
      "source": [
        "# Macro data (CASE 1)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROt8EZiPEHVv",
        "outputId": "dca58432-83ce-4e8b-aa0d-474c9dde3163"
      },
      "source": [
        "look_back = 6\n",
        "trainX = np.reshape(X_macro_train, (X_macro_train.shape[0], 1, X_macro_train.shape[1]))\n",
        "testX = np.reshape(X_macro_test, (X_macro_test.shape[0], 1, X_macro_test.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1, look_back)))\n",
        "# model.add(LSTM(4, input_shape=(1, look_back), return_sequences = True))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.fit(trainX, y_train, epochs=1000, batch_size=10, verbose=2)\n",
        "\n",
        "# evaluate on test set\n",
        "preds = model.predict(testX)\n",
        "\n",
        "preds1=np.reshape(preds,(preds.shape[0],))\n",
        "\n",
        "case1 = np.concatenate((y_train,preds1),axis=0)\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "15/15 - 1s - loss: 7110.5894\n",
            "Epoch 2/1000\n",
            "15/15 - 0s - loss: 7095.9849\n",
            "Epoch 3/1000\n",
            "15/15 - 0s - loss: 7078.9297\n",
            "Epoch 4/1000\n",
            "15/15 - 0s - loss: 7058.6230\n",
            "Epoch 5/1000\n",
            "15/15 - 0s - loss: 7032.1543\n",
            "Epoch 6/1000\n",
            "15/15 - 0s - loss: 7000.5913\n",
            "Epoch 7/1000\n",
            "15/15 - 0s - loss: 6961.2979\n",
            "Epoch 8/1000\n",
            "15/15 - 0s - loss: 6913.7319\n",
            "Epoch 9/1000\n",
            "15/15 - 0s - loss: 6858.7842\n",
            "Epoch 10/1000\n",
            "15/15 - 0s - loss: 6796.8062\n",
            "Epoch 11/1000\n",
            "15/15 - 0s - loss: 6727.2681\n",
            "Epoch 12/1000\n",
            "15/15 - 0s - loss: 6652.4883\n",
            "Epoch 13/1000\n",
            "15/15 - 0s - loss: 6572.3345\n",
            "Epoch 14/1000\n",
            "15/15 - 0s - loss: 6487.8301\n",
            "Epoch 15/1000\n",
            "15/15 - 0s - loss: 6399.7192\n",
            "Epoch 16/1000\n",
            "15/15 - 0s - loss: 6310.6475\n",
            "Epoch 17/1000\n",
            "15/15 - 0s - loss: 6217.1011\n",
            "Epoch 18/1000\n",
            "15/15 - 0s - loss: 6123.4102\n",
            "Epoch 19/1000\n",
            "15/15 - 0s - loss: 6029.2446\n",
            "Epoch 20/1000\n",
            "15/15 - 0s - loss: 5933.5498\n",
            "Epoch 21/1000\n",
            "15/15 - 0s - loss: 5837.6016\n",
            "Epoch 22/1000\n",
            "15/15 - 0s - loss: 5741.4609\n",
            "Epoch 23/1000\n",
            "15/15 - 0s - loss: 5643.9985\n",
            "Epoch 24/1000\n",
            "15/15 - 0s - loss: 5547.3325\n",
            "Epoch 25/1000\n",
            "15/15 - 0s - loss: 5450.7397\n",
            "Epoch 26/1000\n",
            "15/15 - 0s - loss: 5354.5132\n",
            "Epoch 27/1000\n",
            "15/15 - 0s - loss: 5257.3223\n",
            "Epoch 28/1000\n",
            "15/15 - 0s - loss: 5160.8774\n",
            "Epoch 29/1000\n",
            "15/15 - 0s - loss: 5066.1328\n",
            "Epoch 30/1000\n",
            "15/15 - 0s - loss: 4970.6582\n",
            "Epoch 31/1000\n",
            "15/15 - 0s - loss: 4876.3022\n",
            "Epoch 32/1000\n",
            "15/15 - 0s - loss: 4782.7598\n",
            "Epoch 33/1000\n",
            "15/15 - 0s - loss: 4690.3062\n",
            "Epoch 34/1000\n",
            "15/15 - 0s - loss: 4596.9272\n",
            "Epoch 35/1000\n",
            "15/15 - 0s - loss: 4505.7168\n",
            "Epoch 36/1000\n",
            "15/15 - 0s - loss: 4414.8779\n",
            "Epoch 37/1000\n",
            "15/15 - 0s - loss: 4325.7568\n",
            "Epoch 38/1000\n",
            "15/15 - 0s - loss: 4237.2866\n",
            "Epoch 39/1000\n",
            "15/15 - 0s - loss: 4149.1445\n",
            "Epoch 40/1000\n",
            "15/15 - 0s - loss: 4061.5925\n",
            "Epoch 41/1000\n",
            "15/15 - 0s - loss: 3975.6663\n",
            "Epoch 42/1000\n",
            "15/15 - 0s - loss: 3891.1819\n",
            "Epoch 43/1000\n",
            "15/15 - 0s - loss: 3807.0369\n",
            "Epoch 44/1000\n",
            "15/15 - 0s - loss: 3723.2668\n",
            "Epoch 45/1000\n",
            "15/15 - 0s - loss: 3641.7432\n",
            "Epoch 46/1000\n",
            "15/15 - 0s - loss: 3558.8091\n",
            "Epoch 47/1000\n",
            "15/15 - 0s - loss: 3477.5620\n",
            "Epoch 48/1000\n",
            "15/15 - 0s - loss: 3397.2280\n",
            "Epoch 49/1000\n",
            "15/15 - 0s - loss: 3316.7207\n",
            "Epoch 50/1000\n",
            "15/15 - 0s - loss: 3237.6597\n",
            "Epoch 51/1000\n",
            "15/15 - 0s - loss: 3159.5122\n",
            "Epoch 52/1000\n",
            "15/15 - 0s - loss: 3081.9456\n",
            "Epoch 53/1000\n",
            "15/15 - 0s - loss: 3005.0837\n",
            "Epoch 54/1000\n",
            "15/15 - 0s - loss: 2928.6057\n",
            "Epoch 55/1000\n",
            "15/15 - 0s - loss: 2852.1194\n",
            "Epoch 56/1000\n",
            "15/15 - 0s - loss: 2776.6721\n",
            "Epoch 57/1000\n",
            "15/15 - 0s - loss: 2701.5850\n",
            "Epoch 58/1000\n",
            "15/15 - 0s - loss: 2628.1396\n",
            "Epoch 59/1000\n",
            "15/15 - 0s - loss: 2555.6809\n",
            "Epoch 60/1000\n",
            "15/15 - 0s - loss: 2481.7563\n",
            "Epoch 61/1000\n",
            "15/15 - 0s - loss: 2409.3721\n",
            "Epoch 62/1000\n",
            "15/15 - 0s - loss: 2338.5769\n",
            "Epoch 63/1000\n",
            "15/15 - 0s - loss: 2266.8623\n",
            "Epoch 64/1000\n",
            "15/15 - 0s - loss: 2195.9407\n",
            "Epoch 65/1000\n",
            "15/15 - 0s - loss: 2127.1707\n",
            "Epoch 66/1000\n",
            "15/15 - 0s - loss: 2058.0554\n",
            "Epoch 67/1000\n",
            "15/15 - 0s - loss: 1991.0015\n",
            "Epoch 68/1000\n",
            "15/15 - 0s - loss: 1923.8756\n",
            "Epoch 69/1000\n",
            "15/15 - 0s - loss: 1859.3390\n",
            "Epoch 70/1000\n",
            "15/15 - 0s - loss: 1796.8850\n",
            "Epoch 71/1000\n",
            "15/15 - 0s - loss: 1735.2267\n",
            "Epoch 72/1000\n",
            "15/15 - 0s - loss: 1675.3805\n",
            "Epoch 73/1000\n",
            "15/15 - 0s - loss: 1615.9449\n",
            "Epoch 74/1000\n",
            "15/15 - 0s - loss: 1560.4058\n",
            "Epoch 75/1000\n",
            "15/15 - 0s - loss: 1503.1177\n",
            "Epoch 76/1000\n",
            "15/15 - 0s - loss: 1448.2708\n",
            "Epoch 77/1000\n",
            "15/15 - 0s - loss: 1394.6686\n",
            "Epoch 78/1000\n",
            "15/15 - 0s - loss: 1346.4087\n",
            "Epoch 79/1000\n",
            "15/15 - 0s - loss: 1295.1506\n",
            "Epoch 80/1000\n",
            "15/15 - 0s - loss: 1248.2054\n",
            "Epoch 81/1000\n",
            "15/15 - 0s - loss: 1201.5483\n",
            "Epoch 82/1000\n",
            "15/15 - 0s - loss: 1156.6372\n",
            "Epoch 83/1000\n",
            "15/15 - 0s - loss: 1114.5610\n",
            "Epoch 84/1000\n",
            "15/15 - 0s - loss: 1073.6555\n",
            "Epoch 85/1000\n",
            "15/15 - 0s - loss: 1033.3346\n",
            "Epoch 86/1000\n",
            "15/15 - 0s - loss: 994.7700\n",
            "Epoch 87/1000\n",
            "15/15 - 0s - loss: 957.9986\n",
            "Epoch 88/1000\n",
            "15/15 - 0s - loss: 921.1979\n",
            "Epoch 89/1000\n",
            "15/15 - 0s - loss: 887.4265\n",
            "Epoch 90/1000\n",
            "15/15 - 0s - loss: 854.4153\n",
            "Epoch 91/1000\n",
            "15/15 - 0s - loss: 822.5911\n",
            "Epoch 92/1000\n",
            "15/15 - 0s - loss: 792.0326\n",
            "Epoch 93/1000\n",
            "15/15 - 0s - loss: 762.7501\n",
            "Epoch 94/1000\n",
            "15/15 - 0s - loss: 735.3964\n",
            "Epoch 95/1000\n",
            "15/15 - 0s - loss: 708.5466\n",
            "Epoch 96/1000\n",
            "15/15 - 0s - loss: 682.9442\n",
            "Epoch 97/1000\n",
            "15/15 - 0s - loss: 658.4872\n",
            "Epoch 98/1000\n",
            "15/15 - 0s - loss: 635.5979\n",
            "Epoch 99/1000\n",
            "15/15 - 0s - loss: 612.2347\n",
            "Epoch 100/1000\n",
            "15/15 - 0s - loss: 591.6246\n",
            "Epoch 101/1000\n",
            "15/15 - 0s - loss: 570.1819\n",
            "Epoch 102/1000\n",
            "15/15 - 0s - loss: 550.6613\n",
            "Epoch 103/1000\n",
            "15/15 - 0s - loss: 531.4232\n",
            "Epoch 104/1000\n",
            "15/15 - 0s - loss: 513.2484\n",
            "Epoch 105/1000\n",
            "15/15 - 0s - loss: 495.4226\n",
            "Epoch 106/1000\n",
            "15/15 - 0s - loss: 479.2291\n",
            "Epoch 107/1000\n",
            "15/15 - 0s - loss: 463.2379\n",
            "Epoch 108/1000\n",
            "15/15 - 0s - loss: 448.4125\n",
            "Epoch 109/1000\n",
            "15/15 - 0s - loss: 433.6693\n",
            "Epoch 110/1000\n",
            "15/15 - 0s - loss: 420.0182\n",
            "Epoch 111/1000\n",
            "15/15 - 0s - loss: 406.9138\n",
            "Epoch 112/1000\n",
            "15/15 - 0s - loss: 394.1290\n",
            "Epoch 113/1000\n",
            "15/15 - 0s - loss: 382.3751\n",
            "Epoch 114/1000\n",
            "15/15 - 0s - loss: 370.6662\n",
            "Epoch 115/1000\n",
            "15/15 - 0s - loss: 359.7187\n",
            "Epoch 116/1000\n",
            "15/15 - 0s - loss: 349.0500\n",
            "Epoch 117/1000\n",
            "15/15 - 0s - loss: 338.8178\n",
            "Epoch 118/1000\n",
            "15/15 - 0s - loss: 329.2985\n",
            "Epoch 119/1000\n",
            "15/15 - 0s - loss: 319.5987\n",
            "Epoch 120/1000\n",
            "15/15 - 0s - loss: 310.6634\n",
            "Epoch 121/1000\n",
            "15/15 - 0s - loss: 302.3649\n",
            "Epoch 122/1000\n",
            "15/15 - 0s - loss: 294.3349\n",
            "Epoch 123/1000\n",
            "15/15 - 0s - loss: 286.3497\n",
            "Epoch 124/1000\n",
            "15/15 - 0s - loss: 278.9877\n",
            "Epoch 125/1000\n",
            "15/15 - 0s - loss: 272.2099\n",
            "Epoch 126/1000\n",
            "15/15 - 0s - loss: 265.1816\n",
            "Epoch 127/1000\n",
            "15/15 - 0s - loss: 258.4738\n",
            "Epoch 128/1000\n",
            "15/15 - 0s - loss: 252.5645\n",
            "Epoch 129/1000\n",
            "15/15 - 0s - loss: 246.7823\n",
            "Epoch 130/1000\n",
            "15/15 - 0s - loss: 241.0838\n",
            "Epoch 131/1000\n",
            "15/15 - 0s - loss: 235.7763\n",
            "Epoch 132/1000\n",
            "15/15 - 0s - loss: 230.6548\n",
            "Epoch 133/1000\n",
            "15/15 - 0s - loss: 225.3345\n",
            "Epoch 134/1000\n",
            "15/15 - 0s - loss: 220.4717\n",
            "Epoch 135/1000\n",
            "15/15 - 0s - loss: 215.8895\n",
            "Epoch 136/1000\n",
            "15/15 - 0s - loss: 211.3304\n",
            "Epoch 137/1000\n",
            "15/15 - 0s - loss: 207.2177\n",
            "Epoch 138/1000\n",
            "15/15 - 0s - loss: 203.0889\n",
            "Epoch 139/1000\n",
            "15/15 - 0s - loss: 199.1908\n",
            "Epoch 140/1000\n",
            "15/15 - 0s - loss: 195.4516\n",
            "Epoch 141/1000\n",
            "15/15 - 0s - loss: 191.8331\n",
            "Epoch 142/1000\n",
            "15/15 - 0s - loss: 188.4819\n",
            "Epoch 143/1000\n",
            "15/15 - 0s - loss: 185.1174\n",
            "Epoch 144/1000\n",
            "15/15 - 0s - loss: 181.9377\n",
            "Epoch 145/1000\n",
            "15/15 - 0s - loss: 178.8622\n",
            "Epoch 146/1000\n",
            "15/15 - 0s - loss: 175.8482\n",
            "Epoch 147/1000\n",
            "15/15 - 0s - loss: 172.9291\n",
            "Epoch 148/1000\n",
            "15/15 - 0s - loss: 170.0695\n",
            "Epoch 149/1000\n",
            "15/15 - 0s - loss: 167.3960\n",
            "Epoch 150/1000\n",
            "15/15 - 0s - loss: 164.8710\n",
            "Epoch 151/1000\n",
            "15/15 - 0s - loss: 162.5266\n",
            "Epoch 152/1000\n",
            "15/15 - 0s - loss: 160.0249\n",
            "Epoch 153/1000\n",
            "15/15 - 0s - loss: 157.5284\n",
            "Epoch 154/1000\n",
            "15/15 - 0s - loss: 155.3246\n",
            "Epoch 155/1000\n",
            "15/15 - 0s - loss: 153.0504\n",
            "Epoch 156/1000\n",
            "15/15 - 0s - loss: 151.0757\n",
            "Epoch 157/1000\n",
            "15/15 - 0s - loss: 148.7997\n",
            "Epoch 158/1000\n",
            "15/15 - 0s - loss: 146.7395\n",
            "Epoch 159/1000\n",
            "15/15 - 0s - loss: 144.7193\n",
            "Epoch 160/1000\n",
            "15/15 - 0s - loss: 142.8513\n",
            "Epoch 161/1000\n",
            "15/15 - 0s - loss: 140.9536\n",
            "Epoch 162/1000\n",
            "15/15 - 0s - loss: 139.1661\n",
            "Epoch 163/1000\n",
            "15/15 - 0s - loss: 137.1856\n",
            "Epoch 164/1000\n",
            "15/15 - 0s - loss: 135.3708\n",
            "Epoch 165/1000\n",
            "15/15 - 0s - loss: 133.5726\n",
            "Epoch 166/1000\n",
            "15/15 - 0s - loss: 131.9029\n",
            "Epoch 167/1000\n",
            "15/15 - 0s - loss: 130.4158\n",
            "Epoch 168/1000\n",
            "15/15 - 0s - loss: 128.4821\n",
            "Epoch 169/1000\n",
            "15/15 - 0s - loss: 126.9336\n",
            "Epoch 170/1000\n",
            "15/15 - 0s - loss: 125.2932\n",
            "Epoch 171/1000\n",
            "15/15 - 0s - loss: 123.7150\n",
            "Epoch 172/1000\n",
            "15/15 - 0s - loss: 122.2259\n",
            "Epoch 173/1000\n",
            "15/15 - 0s - loss: 120.8860\n",
            "Epoch 174/1000\n",
            "15/15 - 0s - loss: 119.4285\n",
            "Epoch 175/1000\n",
            "15/15 - 0s - loss: 117.9364\n",
            "Epoch 176/1000\n",
            "15/15 - 0s - loss: 116.4750\n",
            "Epoch 177/1000\n",
            "15/15 - 0s - loss: 115.0088\n",
            "Epoch 178/1000\n",
            "15/15 - 0s - loss: 113.7487\n",
            "Epoch 179/1000\n",
            "15/15 - 0s - loss: 112.3083\n",
            "Epoch 180/1000\n",
            "15/15 - 0s - loss: 111.0976\n",
            "Epoch 181/1000\n",
            "15/15 - 0s - loss: 109.8792\n",
            "Epoch 182/1000\n",
            "15/15 - 0s - loss: 108.6602\n",
            "Epoch 183/1000\n",
            "15/15 - 0s - loss: 107.3186\n",
            "Epoch 184/1000\n",
            "15/15 - 0s - loss: 106.2167\n",
            "Epoch 185/1000\n",
            "15/15 - 0s - loss: 104.9669\n",
            "Epoch 186/1000\n",
            "15/15 - 0s - loss: 103.8329\n",
            "Epoch 187/1000\n",
            "15/15 - 0s - loss: 102.6043\n",
            "Epoch 188/1000\n",
            "15/15 - 0s - loss: 101.4776\n",
            "Epoch 189/1000\n",
            "15/15 - 0s - loss: 100.3507\n",
            "Epoch 190/1000\n",
            "15/15 - 0s - loss: 99.2355\n",
            "Epoch 191/1000\n",
            "15/15 - 0s - loss: 98.1880\n",
            "Epoch 192/1000\n",
            "15/15 - 0s - loss: 97.1043\n",
            "Epoch 193/1000\n",
            "15/15 - 0s - loss: 96.1393\n",
            "Epoch 194/1000\n",
            "15/15 - 0s - loss: 95.1141\n",
            "Epoch 195/1000\n",
            "15/15 - 0s - loss: 93.9949\n",
            "Epoch 196/1000\n",
            "15/15 - 0s - loss: 93.1082\n",
            "Epoch 197/1000\n",
            "15/15 - 0s - loss: 92.1438\n",
            "Epoch 198/1000\n",
            "15/15 - 0s - loss: 91.1922\n",
            "Epoch 199/1000\n",
            "15/15 - 0s - loss: 90.2559\n",
            "Epoch 200/1000\n",
            "15/15 - 0s - loss: 89.3112\n",
            "Epoch 201/1000\n",
            "15/15 - 0s - loss: 88.4598\n",
            "Epoch 202/1000\n",
            "15/15 - 0s - loss: 87.6450\n",
            "Epoch 203/1000\n",
            "15/15 - 0s - loss: 86.6088\n",
            "Epoch 204/1000\n",
            "15/15 - 0s - loss: 85.7864\n",
            "Epoch 205/1000\n",
            "15/15 - 0s - loss: 84.9736\n",
            "Epoch 206/1000\n",
            "15/15 - 0s - loss: 84.1059\n",
            "Epoch 207/1000\n",
            "15/15 - 0s - loss: 83.3640\n",
            "Epoch 208/1000\n",
            "15/15 - 0s - loss: 82.5716\n",
            "Epoch 209/1000\n",
            "15/15 - 0s - loss: 81.7597\n",
            "Epoch 210/1000\n",
            "15/15 - 0s - loss: 80.9473\n",
            "Epoch 211/1000\n",
            "15/15 - 0s - loss: 80.2394\n",
            "Epoch 212/1000\n",
            "15/15 - 0s - loss: 79.4801\n",
            "Epoch 213/1000\n",
            "15/15 - 0s - loss: 78.7489\n",
            "Epoch 214/1000\n",
            "15/15 - 0s - loss: 78.0066\n",
            "Epoch 215/1000\n",
            "15/15 - 0s - loss: 77.3004\n",
            "Epoch 216/1000\n",
            "15/15 - 0s - loss: 76.6108\n",
            "Epoch 217/1000\n",
            "15/15 - 0s - loss: 75.9534\n",
            "Epoch 218/1000\n",
            "15/15 - 0s - loss: 75.2910\n",
            "Epoch 219/1000\n",
            "15/15 - 0s - loss: 74.6368\n",
            "Epoch 220/1000\n",
            "15/15 - 0s - loss: 74.0331\n",
            "Epoch 221/1000\n",
            "15/15 - 0s - loss: 73.3671\n",
            "Epoch 222/1000\n",
            "15/15 - 0s - loss: 72.7896\n",
            "Epoch 223/1000\n",
            "15/15 - 0s - loss: 72.1073\n",
            "Epoch 224/1000\n",
            "15/15 - 0s - loss: 71.5743\n",
            "Epoch 225/1000\n",
            "15/15 - 0s - loss: 70.9177\n",
            "Epoch 226/1000\n",
            "15/15 - 0s - loss: 70.3103\n",
            "Epoch 227/1000\n",
            "15/15 - 0s - loss: 69.7212\n",
            "Epoch 228/1000\n",
            "15/15 - 0s - loss: 69.2038\n",
            "Epoch 229/1000\n",
            "15/15 - 0s - loss: 68.6216\n",
            "Epoch 230/1000\n",
            "15/15 - 0s - loss: 68.0972\n",
            "Epoch 231/1000\n",
            "15/15 - 0s - loss: 67.5174\n",
            "Epoch 232/1000\n",
            "15/15 - 0s - loss: 66.9593\n",
            "Epoch 233/1000\n",
            "15/15 - 0s - loss: 66.4107\n",
            "Epoch 234/1000\n",
            "15/15 - 0s - loss: 65.9442\n",
            "Epoch 235/1000\n",
            "15/15 - 0s - loss: 65.4150\n",
            "Epoch 236/1000\n",
            "15/15 - 0s - loss: 64.9689\n",
            "Epoch 237/1000\n",
            "15/15 - 0s - loss: 64.4834\n",
            "Epoch 238/1000\n",
            "15/15 - 0s - loss: 63.8934\n",
            "Epoch 239/1000\n",
            "15/15 - 0s - loss: 63.4305\n",
            "Epoch 240/1000\n",
            "15/15 - 0s - loss: 62.9202\n",
            "Epoch 241/1000\n",
            "15/15 - 0s - loss: 62.4621\n",
            "Epoch 242/1000\n",
            "15/15 - 0s - loss: 61.9870\n",
            "Epoch 243/1000\n",
            "15/15 - 0s - loss: 61.4911\n",
            "Epoch 244/1000\n",
            "15/15 - 0s - loss: 61.0508\n",
            "Epoch 245/1000\n",
            "15/15 - 0s - loss: 60.6557\n",
            "Epoch 246/1000\n",
            "15/15 - 0s - loss: 60.1590\n",
            "Epoch 247/1000\n",
            "15/15 - 0s - loss: 59.7618\n",
            "Epoch 248/1000\n",
            "15/15 - 0s - loss: 59.2612\n",
            "Epoch 249/1000\n",
            "15/15 - 0s - loss: 58.8560\n",
            "Epoch 250/1000\n",
            "15/15 - 0s - loss: 58.4090\n",
            "Epoch 251/1000\n",
            "15/15 - 0s - loss: 57.9716\n",
            "Epoch 252/1000\n",
            "15/15 - 0s - loss: 57.6207\n",
            "Epoch 253/1000\n",
            "15/15 - 0s - loss: 57.2321\n",
            "Epoch 254/1000\n",
            "15/15 - 0s - loss: 56.7780\n",
            "Epoch 255/1000\n",
            "15/15 - 0s - loss: 56.4113\n",
            "Epoch 256/1000\n",
            "15/15 - 0s - loss: 56.0709\n",
            "Epoch 257/1000\n",
            "15/15 - 0s - loss: 55.7371\n",
            "Epoch 258/1000\n",
            "15/15 - 0s - loss: 55.3610\n",
            "Epoch 259/1000\n",
            "15/15 - 0s - loss: 55.0296\n",
            "Epoch 260/1000\n",
            "15/15 - 0s - loss: 54.6495\n",
            "Epoch 261/1000\n",
            "15/15 - 0s - loss: 54.3191\n",
            "Epoch 262/1000\n",
            "15/15 - 0s - loss: 53.9975\n",
            "Epoch 263/1000\n",
            "15/15 - 0s - loss: 53.6729\n",
            "Epoch 264/1000\n",
            "15/15 - 0s - loss: 53.2708\n",
            "Epoch 265/1000\n",
            "15/15 - 0s - loss: 52.9198\n",
            "Epoch 266/1000\n",
            "15/15 - 0s - loss: 52.6258\n",
            "Epoch 267/1000\n",
            "15/15 - 0s - loss: 52.2986\n",
            "Epoch 268/1000\n",
            "15/15 - 0s - loss: 52.0147\n",
            "Epoch 269/1000\n",
            "15/15 - 0s - loss: 51.7355\n",
            "Epoch 270/1000\n",
            "15/15 - 0s - loss: 51.4699\n",
            "Epoch 271/1000\n",
            "15/15 - 0s - loss: 51.1990\n",
            "Epoch 272/1000\n",
            "15/15 - 0s - loss: 50.8907\n",
            "Epoch 273/1000\n",
            "15/15 - 0s - loss: 50.5926\n",
            "Epoch 274/1000\n",
            "15/15 - 0s - loss: 50.3133\n",
            "Epoch 275/1000\n",
            "15/15 - 0s - loss: 50.0472\n",
            "Epoch 276/1000\n",
            "15/15 - 0s - loss: 49.7939\n",
            "Epoch 277/1000\n",
            "15/15 - 0s - loss: 49.5075\n",
            "Epoch 278/1000\n",
            "15/15 - 0s - loss: 49.2206\n",
            "Epoch 279/1000\n",
            "15/15 - 0s - loss: 49.0066\n",
            "Epoch 280/1000\n",
            "15/15 - 0s - loss: 48.7549\n",
            "Epoch 281/1000\n",
            "15/15 - 0s - loss: 48.4881\n",
            "Epoch 282/1000\n",
            "15/15 - 0s - loss: 48.2556\n",
            "Epoch 283/1000\n",
            "15/15 - 0s - loss: 48.0333\n",
            "Epoch 284/1000\n",
            "15/15 - 0s - loss: 47.8090\n",
            "Epoch 285/1000\n",
            "15/15 - 0s - loss: 47.6004\n",
            "Epoch 286/1000\n",
            "15/15 - 0s - loss: 47.3631\n",
            "Epoch 287/1000\n",
            "15/15 - 0s - loss: 47.1629\n",
            "Epoch 288/1000\n",
            "15/15 - 0s - loss: 46.9649\n",
            "Epoch 289/1000\n",
            "15/15 - 0s - loss: 46.7304\n",
            "Epoch 290/1000\n",
            "15/15 - 0s - loss: 46.5283\n",
            "Epoch 291/1000\n",
            "15/15 - 0s - loss: 46.4032\n",
            "Epoch 292/1000\n",
            "15/15 - 0s - loss: 46.1136\n",
            "Epoch 293/1000\n",
            "15/15 - 0s - loss: 45.9806\n",
            "Epoch 294/1000\n",
            "15/15 - 0s - loss: 45.8910\n",
            "Epoch 295/1000\n",
            "15/15 - 0s - loss: 45.6095\n",
            "Epoch 296/1000\n",
            "15/15 - 0s - loss: 45.4046\n",
            "Epoch 297/1000\n",
            "15/15 - 0s - loss: 45.2394\n",
            "Epoch 298/1000\n",
            "15/15 - 0s - loss: 45.0238\n",
            "Epoch 299/1000\n",
            "15/15 - 0s - loss: 44.8488\n",
            "Epoch 300/1000\n",
            "15/15 - 0s - loss: 44.6831\n",
            "Epoch 301/1000\n",
            "15/15 - 0s - loss: 44.5127\n",
            "Epoch 302/1000\n",
            "15/15 - 0s - loss: 44.3596\n",
            "Epoch 303/1000\n",
            "15/15 - 0s - loss: 44.2232\n",
            "Epoch 304/1000\n",
            "15/15 - 0s - loss: 44.0846\n",
            "Epoch 305/1000\n",
            "15/15 - 0s - loss: 43.8830\n",
            "Epoch 306/1000\n",
            "15/15 - 0s - loss: 43.7076\n",
            "Epoch 307/1000\n",
            "15/15 - 0s - loss: 43.5265\n",
            "Epoch 308/1000\n",
            "15/15 - 0s - loss: 43.3855\n",
            "Epoch 309/1000\n",
            "15/15 - 0s - loss: 43.1837\n",
            "Epoch 310/1000\n",
            "15/15 - 0s - loss: 43.0758\n",
            "Epoch 311/1000\n",
            "15/15 - 0s - loss: 42.8896\n",
            "Epoch 312/1000\n",
            "15/15 - 0s - loss: 42.7337\n",
            "Epoch 313/1000\n",
            "15/15 - 0s - loss: 42.6097\n",
            "Epoch 314/1000\n",
            "15/15 - 0s - loss: 42.4514\n",
            "Epoch 315/1000\n",
            "15/15 - 0s - loss: 42.3648\n",
            "Epoch 316/1000\n",
            "15/15 - 0s - loss: 42.1810\n",
            "Epoch 317/1000\n",
            "15/15 - 0s - loss: 42.0448\n",
            "Epoch 318/1000\n",
            "15/15 - 0s - loss: 41.8721\n",
            "Epoch 319/1000\n",
            "15/15 - 0s - loss: 41.7576\n",
            "Epoch 320/1000\n",
            "15/15 - 0s - loss: 41.6585\n",
            "Epoch 321/1000\n",
            "15/15 - 0s - loss: 41.5755\n",
            "Epoch 322/1000\n",
            "15/15 - 0s - loss: 41.4167\n",
            "Epoch 323/1000\n",
            "15/15 - 0s - loss: 41.2187\n",
            "Epoch 324/1000\n",
            "15/15 - 0s - loss: 41.0877\n",
            "Epoch 325/1000\n",
            "15/15 - 0s - loss: 40.9492\n",
            "Epoch 326/1000\n",
            "15/15 - 0s - loss: 40.8021\n",
            "Epoch 327/1000\n",
            "15/15 - 0s - loss: 40.6915\n",
            "Epoch 328/1000\n",
            "15/15 - 0s - loss: 40.5813\n",
            "Epoch 329/1000\n",
            "15/15 - 0s - loss: 40.4190\n",
            "Epoch 330/1000\n",
            "15/15 - 0s - loss: 40.3371\n",
            "Epoch 331/1000\n",
            "15/15 - 0s - loss: 40.1589\n",
            "Epoch 332/1000\n",
            "15/15 - 0s - loss: 40.0526\n",
            "Epoch 333/1000\n",
            "15/15 - 0s - loss: 39.9993\n",
            "Epoch 334/1000\n",
            "15/15 - 0s - loss: 39.8427\n",
            "Epoch 335/1000\n",
            "15/15 - 0s - loss: 39.6872\n",
            "Epoch 336/1000\n",
            "15/15 - 0s - loss: 39.5818\n",
            "Epoch 337/1000\n",
            "15/15 - 0s - loss: 39.5058\n",
            "Epoch 338/1000\n",
            "15/15 - 0s - loss: 39.4150\n",
            "Epoch 339/1000\n",
            "15/15 - 0s - loss: 39.2651\n",
            "Epoch 340/1000\n",
            "15/15 - 0s - loss: 39.1048\n",
            "Epoch 341/1000\n",
            "15/15 - 0s - loss: 39.0064\n",
            "Epoch 342/1000\n",
            "15/15 - 0s - loss: 38.9185\n",
            "Epoch 343/1000\n",
            "15/15 - 0s - loss: 38.8488\n",
            "Epoch 344/1000\n",
            "15/15 - 0s - loss: 38.6701\n",
            "Epoch 345/1000\n",
            "15/15 - 0s - loss: 38.5254\n",
            "Epoch 346/1000\n",
            "15/15 - 0s - loss: 38.4362\n",
            "Epoch 347/1000\n",
            "15/15 - 0s - loss: 38.3890\n",
            "Epoch 348/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuDpDO1OEHVv"
      },
      "source": [
        "# Micro data (CASE 2)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PzJXQ_PEHVv"
      },
      "source": [
        "look_back = 6\n",
        "trainX = np.reshape(X_micro_train, (X_micro_train.shape[0], 1, X_micro_train.shape[1]))\n",
        "testX = np.reshape(X_micro_test, (X_micro_test.shape[0], 1, X_micro_test.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1, look_back)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.fit(trainX, y_train, epochs=1000, batch_size=10, verbose=2)\n",
        "\n",
        "# evaluate on test set\n",
        "preds = model.predict(testX)\n",
        "\n",
        "preds2=np.reshape(preds,(preds.shape[0],))\n",
        "\n",
        "case2 = np.concatenate((y_train,preds2),axis=0)\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LULxx_2sEHVv"
      },
      "source": [
        "# All data (CASE 3)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QACn29A-EHVw"
      },
      "source": [
        "look_back = 12\n",
        "X_train=X[:144,]\n",
        "X_test=X[144:,]\n",
        "trainX = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "testX = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1, look_back)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.fit(trainX, y_train, epochs=1000, batch_size=10, verbose=2)\n",
        "\n",
        "# evaluate on test set\n",
        "preds = model.predict(testX)\n",
        "\n",
        "preds3=np.reshape(preds,(preds.shape[0],))\n",
        "\n",
        "case3 = np.concatenate((y_train,preds3),axis=0)\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCqj77ChEHVw"
      },
      "source": [
        "# matplot graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuuktI0aEHVw"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "x_date = df.iloc[:,0]\n",
        "plt.plot(x_date,case1, color=\"salmon\")\n",
        "plt.plot(x_date,case2, color=\"seagreen\")\n",
        "plt.plot(x_date,case3, color=\"deeppink\")\n",
        "plt.plot(x_date,y, color=\"black\")\n",
        "plt.legend(['case 1','case 2','case 3', 'real'])\n",
        "plt.xticks(ticks=x_date, rotation=45)\n",
        "plt.locator_params(axis='x', nbins=x_date.shape[0]/12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7-GXY9EHVw"
      },
      "source": [
        "arr=[mean_squared_error(y_test,preds1), mean_squared_error(y_test,preds2), mean_squared_error(y_test,preds3)]\n",
        "result=pd.DataFrame(arr).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKrE8zJoEHVw"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-7H-6JhEHVx"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/AI 2021/result_lstm.csv'):\n",
        "    result.to_csv('/content/drive/MyDrive/Colab Notebooks/AI 2021/result_lstm.csv', index=False, mode='w', encoding='utf-8-sig')\n",
        "else:\n",
        "    result.to_csv('/content/drive/MyDrive/Colab Notebooks/AI 2021/result_lstm.csv', index=False, mode='a', encoding='utf-8-sig', header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AKapGItQc_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}